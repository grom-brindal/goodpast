{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMJ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchani\n",
    "import os\n",
    "import math\n",
    "import torch.utils.tensorboard\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchani.units import hartree2kcalmol\n",
    "\n",
    "device = torch.device('cpu')\n",
    "const_file = './train/eval/rHCNO-5.2R_16-3.5A_a4-8.params'\n",
    "sae_file = './train/eval/sae_linfit_CAS.dat'\n",
    "\n",
    "consts = torchani.neurochem.Constants(const_file)\n",
    "aev_computer = torchani.AEVComputer(**consts)\n",
    "energy_shifter = torchani.neurochem.load_sae(sae_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "output_filename = './train/dataset/small_set/small_set.h5'\n",
    "if os.path.isfile(output_filename):\n",
    "    with h5py.File(output_filename, 'w') as f:\n",
    "        del f[output_filename]\n",
    "        f.close()\n",
    "with h5py.File('./train/dataset/set.h5', 'r') as f:\n",
    "    coordinates = f['firstlayer']['secondlayer']['coordinates']\n",
    "    coordinates = f['firstlayer']['secondlayer']['coordinates'][:len(coordinates)]\n",
    "    dataset_energies = f['firstlayer']['secondlayer']['energies'][:len(coordinates)]\n",
    "    f.close()\n",
    "output_filepath = './train/dataset/small_set/'\n",
    "output_filename ='./train/dataset/small_set/small_set.h5'\n",
    "try:\n",
    "    os.mkdir('./train/dataset/small_set/')\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "output_dataset = h5py.File(output_filename, 'w')\n",
    "layer1 = output_dataset.create_group('firstlayer')\n",
    "layer2 = layer1.create_group('secondlayer')\n",
    "atype = np.array((b'C',b'C',b'H',b'H',b'H',b'H'))\n",
    "layer2.create_dataset('coordinates', data = coordinates[0:10])\n",
    "layer2.create_dataset('energies', data = dataset_energies[0:10])\n",
    "layer2.create_dataset('species', data=atype)\n",
    "with h5py.File('./train/dataset/small_set.h5', 'r') as f:\n",
    "    small_coordinates = f['firstlayer']['secondlayer']['coordinates']\n",
    "    small_coordinates = f['firstlayer']['secondlayer']['coordinates'][:len(coordinates)]\n",
    "    print(len(small_coordinates))\n",
    "    f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self atomic energies:  tensor([-15.6097,  -7.8048], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    path = os.path.dirname(os.path.realpath(__file__))\n",
    "except NameError:\n",
    "    path = os.getcwd()\n",
    "dspath = os.path.join(path, './train/dataset/small_set/small_set.h5')\n",
    "batch_size = 5\n",
    "species_order = ['H', 'C', 'N', 'O']\n",
    "training, validation = torchani.data.load(dspath).subtract_self_energies(energy_shifter).species_to_indices(species_order).shuffle().split(0.8, None)\n",
    "training = training.collate(batch_size).cache()\n",
    "validation = validation.collate(batch_size).cache()\n",
    "print('Self atomic energies: ', energy_shifter.self_energies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANIModel(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=160, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=160, out_features=128, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=128, out_features=96, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=144, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=144, out_features=112, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=128, out_features=112, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=128, out_features=112, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchani' has no attribute 'optim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 87\u001b[0m\n\u001b[0;32m     69\u001b[0m model \u001b[39m=\u001b[39m torchani\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSequential(aev_computer, nn)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     71\u001b[0m \u001b[39m###############################################################################\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m# Now let's setup the optimizers. NeuroChem uses Adam with decoupled weight decay\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m# to updates the weights and Stochastic Gradient Descent (SGD) to update the biases.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m# .. _Decoupled Weight Decay Regularization:\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m#   https://arxiv.org/abs/1711.05101\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m AdamW \u001b[39m=\u001b[39m torchani\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39mAdamW([\n\u001b[0;32m     88\u001b[0m     \u001b[39m# H networks\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [H_network[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m     90\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [H_network[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.00001\u001b[39m},\n\u001b[0;32m     91\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [H_network[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.000001\u001b[39m},\n\u001b[0;32m     92\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [H_network[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m     93\u001b[0m     \u001b[39m# C networks\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [C_network[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m     95\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [C_network[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.00001\u001b[39m},\n\u001b[0;32m     96\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [C_network[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.000001\u001b[39m},\n\u001b[0;32m     97\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [C_network[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m     98\u001b[0m     \u001b[39m# N networks\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [N_network[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m    100\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [N_network[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.00001\u001b[39m},\n\u001b[0;32m    101\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [N_network[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.000001\u001b[39m},\n\u001b[0;32m    102\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [N_network[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m    103\u001b[0m     \u001b[39m# O networks\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [O_network[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m    105\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [O_network[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.00001\u001b[39m},\n\u001b[0;32m    106\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [O_network[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mweight], \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.000001\u001b[39m},\n\u001b[0;32m    107\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [O_network[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mweight]},\n\u001b[0;32m    108\u001b[0m ])\n\u001b[0;32m    110\u001b[0m SGD \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD([\n\u001b[0;32m    111\u001b[0m     \u001b[39m# H networks\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [H_network[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbias]},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: [O_network[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mbias]},\n\u001b[0;32m    131\u001b[0m ], lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[39m###############################################################################\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Setting up a learning rate scheduler to do learning rate decay\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchani' has no attribute 'optim'"
     ]
    }
   ],
   "source": [
    "\n",
    "aev_dim = aev_computer.aev_length\n",
    "\n",
    "H_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 160),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(160, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "C_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 144),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(144, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "N_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "O_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "nn = torchani.ANIModel([H_network, C_network, N_network, O_network])\n",
    "print(nn)\n",
    "\n",
    "###############################################################################\n",
    "# Initialize the weights and biases.\n",
    "#\n",
    "# .. note::\n",
    "#   Pytorch default initialization for the weights and biases in linear layers\n",
    "#   is Kaiming uniform. See: `TORCH.NN.MODULES.LINEAR`_\n",
    "#   We initialize the weights similarly but from the normal distribution.\n",
    "#   The biases were initialized to zero.\n",
    "#\n",
    "# .. _TORCH.NN.MODULES.LINEAR:\n",
    "#   https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "\n",
    "\n",
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "nn.apply(init_params)\n",
    "\n",
    "###############################################################################\n",
    "# Let's now create a pipeline of AEV Computer --> Neural Networks.\n",
    "model = torchani.nn.Sequential(aev_computer, nn).to(device)\n",
    "\n",
    "###############################################################################\n",
    "# Now let's setup the optimizers. NeuroChem uses Adam with decoupled weight decay\n",
    "# to updates the weights and Stochastic Gradient Descent (SGD) to update the biases.\n",
    "# Moreover, we need to specify different weight decay rate for different layes.\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#   The weight decay in `inputtrain.ipt`_ is named \"l2\", but it is actually not\n",
    "#   L2 regularization. The confusion between L2 and weight decay is a common\n",
    "#   mistake in deep learning.  See: `Decoupled Weight Decay Regularization`_\n",
    "#   Also note that the weight decay only applies to weight in the training\n",
    "#   of ANI models, not bias.\n",
    "#\n",
    "# .. _Decoupled Weight Decay Regularization:\n",
    "#   https://arxiv.org/abs/1711.05101\n",
    "\n",
    "AdamW = torchani.optim.AdamW([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].weight]},\n",
    "    {'params': [H_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [H_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [H_network[6].weight]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].weight]},\n",
    "    {'params': [C_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [C_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [C_network[6].weight]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].weight]},\n",
    "    {'params': [N_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [N_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [N_network[6].weight]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].weight]},\n",
    "    {'params': [O_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [O_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [O_network[6].weight]},\n",
    "])\n",
    "\n",
    "SGD = torch.optim.SGD([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].bias]},\n",
    "    {'params': [H_network[2].bias]},\n",
    "    {'params': [H_network[4].bias]},\n",
    "    {'params': [H_network[6].bias]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].bias]},\n",
    "    {'params': [C_network[2].bias]},\n",
    "    {'params': [C_network[4].bias]},\n",
    "    {'params': [C_network[6].bias]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].bias]},\n",
    "    {'params': [N_network[2].bias]},\n",
    "    {'params': [N_network[4].bias]},\n",
    "    {'params': [N_network[6].bias]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].bias]},\n",
    "    {'params': [O_network[2].bias]},\n",
    "    {'params': [O_network[4].bias]},\n",
    "    {'params': [O_network[6].bias]},\n",
    "], lr=1e-3)\n",
    "\n",
    "###############################################################################\n",
    "# Setting up a learning rate scheduler to do learning rate decay\n",
    "AdamW_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamW, factor=0.5, patience=100, threshold=0)\n",
    "SGD_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD, factor=0.5, patience=100, threshold=0)\n",
    "\n",
    "###############################################################################\n",
    "# Train the model by minimizing the MSE loss, until validation RMSE no longer\n",
    "# improves during a certain number of steps, decay the learning rate and repeat\n",
    "# the same process, stop until the learning rate is smaller than a threshold.\n",
    "#\n",
    "# We first read the checkpoint files to restart training. We use `latest.pt`\n",
    "# to store current training state.\n",
    "latest_checkpoint = 'latest.pt'\n",
    "\n",
    "###############################################################################\n",
    "# Resume training from previously saved checkpoints:\n",
    "if os.path.isfile(latest_checkpoint):\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    nn.load_state_dict(checkpoint['nn'])\n",
    "    AdamW.load_state_dict(checkpoint['AdamW'])\n",
    "    SGD.load_state_dict(checkpoint['SGD'])\n",
    "    AdamW_scheduler.load_state_dict(checkpoint['AdamW_scheduler'])\n",
    "    SGD_scheduler.load_state_dict(checkpoint['SGD_scheduler'])\n",
    "\n",
    "###############################################################################\n",
    "# During training, we need to validate on validation set and if validation error\n",
    "# is better than the best, then save the new best model to a checkpoint\n",
    "\n",
    "\n",
    "def validate():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    total_mse = 0.0\n",
    "    count = 0\n",
    "    for properties in validation:\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "        total_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "        count += predicted_energies.shape[0]\n",
    "    return hartree2kcalmol(math.sqrt(total_mse / count))\n",
    "\n",
    "def train_validate():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    total_mse = 0.0\n",
    "    count = 0\n",
    "    for properties in training:\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "        total_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "        count += predicted_energies.shape[0]\n",
    "    return hartree2kcalmol(math.sqrt(total_mse / count))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# We will also use TensorBoard to visualize our training process\n",
    "tensorboard = torch.utils.tensorboard.SummaryWriter()\n",
    "\n",
    "###############################################################################\n",
    "# Finally, we come to the training loop.\n",
    "#\n",
    "# In this tutorial, we are setting the maximum epoch to a very small number,\n",
    "# only to make this demo terminate fast. For serious training, this should be\n",
    "# set to a much larger value\n",
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "print(\"training starting from epoch\", AdamW_scheduler.last_epoch + 1)\n",
    "max_epochs = 10000\n",
    "early_stopping_learning_rate = 1.0E-06\n",
    "best_model_checkpoint = 'best.pt'\n",
    "\n",
    "for _ in range(AdamW_scheduler.last_epoch + 1, max_epochs):\n",
    "    rmse = validate()\n",
    "    train_rmse = train_validate()\n",
    "    print('RMSE:', rmse, 'Train RMSE:', train_rmse, 'at epoch', AdamW_scheduler.last_epoch + 1)\n",
    "\n",
    "    learning_rate = AdamW.param_groups[0]['lr']\n",
    "\n",
    "    if learning_rate < early_stopping_learning_rate:\n",
    "        break\n",
    "\n",
    "    # checkpoint\n",
    "    if AdamW_scheduler.is_better(rmse, AdamW_scheduler.best):\n",
    "        torch.save(nn.state_dict(), best_model_checkpoint)\n",
    "\n",
    "    AdamW_scheduler.step(rmse)\n",
    "    SGD_scheduler.step(rmse)\n",
    "\n",
    "    tensorboard.add_scalar('validation_rmse', rmse, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('best_validation_rmse', AdamW_scheduler.best, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('learning_rate', learning_rate, AdamW_scheduler.last_epoch)\n",
    "\n",
    "    for i, properties in tqdm.tqdm(\n",
    "        enumerate(training),\n",
    "        total=len(training),\n",
    "        desc=\"epoch {}\".format(AdamW_scheduler.last_epoch)\n",
    "    ):\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        num_atoms = (species >= 0).sum(dim=1, dtype=true_energies.dtype)\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "\n",
    "        loss = (mse(predicted_energies, true_energies) / num_atoms.sqrt()).mean()\n",
    "\n",
    "        AdamW.zero_grad()\n",
    "        SGD.zero_grad()\n",
    "        loss.backward()\n",
    "        AdamW.step()\n",
    "        SGD.step()\n",
    "\n",
    "        # write current batch loss to TensorBoard\n",
    "        tensorboard.add_scalar('batch_loss', loss, AdamW_scheduler.last_epoch * len(training) + i)\n",
    "\n",
    "    torch.save({\n",
    "        'nn': nn.state_dict(),\n",
    "        'AdamW': AdamW.state_dict(),\n",
    "        'SGD': SGD.state_dict(),\n",
    "        'AdamW_scheduler': AdamW_scheduler.state_dict(),\n",
    "        'SGD_scheduler': SGD_scheduler.state_dict(),\n",
    "    }, latest_checkpoint)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe0d356910019e44f2687c3afcfb53699a594892937df3d97268ef1ba4671729"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
